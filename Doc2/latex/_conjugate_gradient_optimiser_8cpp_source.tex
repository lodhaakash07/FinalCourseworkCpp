\doxysection{Conjugate\+Gradient\+Optimiser.\+cpp}
\hypertarget{_conjugate_gradient_optimiser_8cpp_source}{}\label{_conjugate_gradient_optimiser_8cpp_source}
\begin{DoxyCode}{0}
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00001}00001\ \textcolor{preprocessor}{\#include\ "{}Headers/ConjugateGradientOptimiser.h"{}}}
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00002}00002\ \textcolor{preprocessor}{\#include\ "{}Headers/Matrix.h"{}}}
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00003}00003\ }
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00004}00004\ std::vector<double>\ ConjugateGradientOptimiser::getWeights(\textcolor{keyword}{const}\ \mbox{\hyperlink{class_matrix}{Matrix}}\&\ covMatrix,\ \textcolor{keyword}{const}\ std::vector<double>\&\ meanReturns,\ \textcolor{keywordtype}{double}\ targetReturn)\ \{}
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00005}00005\ \ \ \ \ \textcolor{comment}{//\ Number\ of\ assets}}
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00006}00006\ \ \ \ \ \textcolor{keywordtype}{int}\ numAssets\ =\ covMatrix.size();}
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00007}00007\ }
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00008}00008\ \ \ \ \ \textcolor{comment}{//\ Initialize\ weights\ vector\ with\ equal\ allocation}}
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00009}00009\ \ \ \ \ std::vector<double>\ weights(numAssets,\ 1.0\ /\ numAssets);}
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00010}00010\ }
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00011}00011\ \ \ \ \ \textcolor{comment}{//\ Conjugate\ Gradient\ iterations}}
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00012}00012\ \ \ \ \ \textcolor{keywordtype}{int}\ maxIterations\ =\ 1000;\ \ \textcolor{comment}{//\ Maximum\ number\ of\ iterations}}
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00013}00013\ \ \ \ \ \textcolor{keywordtype}{double}\ tolerance\ =\ 1e-\/6;\ \ \ \ \ \textcolor{comment}{//\ Tolerance\ for\ convergence}}
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00014}00014\ }
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00015}00015\ \ \ \ \ std::vector<double>\ gradient(numAssets);\ \ \ \ \textcolor{comment}{//\ Gradient\ vector}}
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00016}00016\ \ \ \ \ std::vector<double>\ direction(numAssets);\ \ \ \textcolor{comment}{//\ Conjugate\ direction\ vector}}
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00017}00017\ \ \ \ \ std::vector<double>\ prevDirection(numAssets);\ \ \textcolor{comment}{//\ Previous\ conjugate\ direction\ vector}}
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00018}00018\ }
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00019}00019\ \ \ \ \ \textcolor{keywordtype}{double}\ alpha,\ beta,\ dotProduct,\ normGradient;}
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00020}00020\ }
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00021}00021\ \ \ \ \ \textcolor{keywordflow}{for}\ (\textcolor{keywordtype}{int}\ iteration\ =\ 0;\ iteration\ <\ maxIterations;\ iteration++)\ \{}
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00022}00022\ \ \ \ \ \ \ \ \ \textcolor{comment}{//\ Calculate\ gradient}}
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00023}00023\ \ \ \ \ \ \ \ \ gradient\ =\ covMatrix.multiplyMatrixVector(covMatrix,\ weights);}
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00024}00024\ \ \ \ \ \ \ \ \ gradient\ =\ covMatrix.multiplyVector(2.0,\ gradient);}
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00025}00025\ \ \ \ \ \ \ \ \ std::vector<double>\ meanReturnsScaled\ =\ covMatrix.multiplyVector(2.0\ *\ targetReturn,\ meanReturns);}
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00026}00026\ \ \ \ \ \ \ \ \ gradient\ =\ covMatrix.subtractVectors(gradient,\ meanReturnsScaled);}
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00027}00027\ }
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00028}00028\ \ \ \ \ \ \ \ \ \textcolor{comment}{//\ Check\ for\ convergence}}
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00029}00029\ \ \ \ \ \ \ \ \ normGradient\ =\ covMatrix.norm(gradient);}
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00030}00030\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ (normGradient\ <\ tolerance)\ \{}
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00031}00031\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{break};\ \ \textcolor{comment}{//\ Convergence\ achieved}}
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00032}00032\ \ \ \ \ \ \ \ \ \}}
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00033}00033\ }
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00034}00034\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ (iteration\ ==\ 0)\ \{}
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00035}00035\ \ \ \ \ \ \ \ \ \ \ \ \ direction\ =\ gradient;}
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00036}00036\ \ \ \ \ \ \ \ \ \}\ \textcolor{keywordflow}{else}\ \{}
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00037}00037\ \ \ \ \ \ \ \ \ \ \ \ \ dotProduct\ =\ covMatrix.dotProduct(gradient,\ gradient);}
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00038}00038\ \ \ \ \ \ \ \ \ \ \ \ \ beta\ =\ dotProduct\ /\ normGradient;}
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00039}00039\ }
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00040}00040\ \ \ \ \ \ \ \ \ \ \ \ \ direction\ =\ covMatrix.addVectors(gradient,\ covMatrix.multiplyVector(beta,\ prevDirection));}
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00041}00041\ \ \ \ \ \ \ \ \ \}}
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00042}00042\ }
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00043}00043\ \ \ \ \ \ \ \ \ \textcolor{comment}{//\ Calculate\ step\ size}}
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00044}00044\ \ \ \ \ \ \ \ \ dotProduct\ =\ covMatrix.dotProduct(gradient,\ direction);}
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00045}00045\ \ \ \ \ \ \ \ \ alpha\ =\ normGradient\ /\ dotProduct;}
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00046}00046\ }
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00047}00047\ \ \ \ \ \ \ \ \ \textcolor{comment}{//\ Update\ weights}}
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00048}00048\ \ \ \ \ \ \ \ \ weights\ =\ covMatrix.subtractVectors(weights,\ covMatrix.multiplyVector(alpha,\ direction));}
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00049}00049\ }
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00050}00050\ \ \ \ \ \ \ \ \ \textcolor{comment}{//\ Update\ previous\ direction}}
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00051}00051\ \ \ \ \ \ \ \ \ prevDirection\ =\ direction;}
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00052}00052\ \ \ \ \ \}}
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00053}00053\ }
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00054}00054\ \ \ \ \ \textcolor{keywordflow}{return}\ weights;}
\DoxyCodeLine{\Hypertarget{_conjugate_gradient_optimiser_8cpp_source_l00055}00055\ \}}

\end{DoxyCode}
